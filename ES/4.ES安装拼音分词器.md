# ES安装拼音分词器

在之前的文章中介绍过`ik`分词器如何安装使用，这期介绍`pinyin`分词器如何安装使用。该项目同样是由作者`medcl`写的，具体可以项目地址参考如下：

```url
https://github.com/medcl/elasticsearch-analysis-pinyin
```

该分词器提供了分析器`pinyin`，分词器`pinyin`和分词过滤器`pinyin`。

> 关于他们之间的区别，关注公众号回复“分词器”关键字即可返回分析器相关内容文章。

## 安装方式

安装方式与之前安装`ik`分词器一样，可以通过ES插件安装，也可以自己下载解压安装。我这里只介绍下载解压安装方式，另外一种方式请参考之前的安装`ik`分词器的文章。

> 关于安装ik分词器，关注公众号回复“ik”关键字即可返回安装ik分词器相关内容文章。

安装包下载地址如下：

```url
https://github.com/medcl/elasticsearch-analysis-pinyin/releases
```

请选择ES对应版本的安装包，我这里使用的是`7.1.1`版本。下载完成之后，在`${ES_HOME}/plugins`目录下创建目录`pinyin`，然后解压压缩包将内容拷贝到该文件夹中，然后重启ES服务即可。

## 测试

重启ES之后，我们可以使用`_analyze`API来测试一下效果，测试代码如下：

```http
POST /_analyze
{
  "tokenizer": "pinyin",
  "text": "苹果"
}
```

分词效果如下：

```json
{
  "tokens" : [
    {
      "token" : "ping",
      "start_offset" : 0,
      "end_offset" : 0,
      "type" : "word",
      "position" : 0
    },
    {
      "token" : "pg",
      "start_offset" : 0,
      "end_offset" : 0,
      "type" : "word",
      "position" : 0
    },
    {
      "token" : "guo",
      "start_offset" : 0,
      "end_offset" : 0,
      "type" : "word",
      "position" : 1
    }
  ]
}
```

从响应结果来看，拼音分词器会将我们的文本中的字一个个拆开，然后装换成拼音，同时还会获取每个字拼音的首字母将其组成一个词。

## 属性介绍

上面的示例是拼音分词器默认的效果，与`ik`分词器不同的是，`pinyin`分词器通过很多属性去控制分词效果。下面来试验不同属性所带来的效果。

### keep_first_letter

该属性的效果就是将每个词的首字母组成一个词。例如我们示例中的`苹果`->`pg`。默认情况下该属性是`true`。

下面我们将该效果关闭，再来测试结果如何。这里我们首先需要自定义一个分析器，关于如何自定义分析器后续会再写文章介绍，这里暂时明白我们自定义分词器即可。

```http
PUT pinyin
{
  "settings": {
    "analysis": {

      "tokenizer": {
        "my_pinyin":{
          "type":"pinyin",
          "keep_first_letter":false
        }
      },

      "analyzer": {
        "pinyin_analyzer":{
          "tokenizer":"my_pinyin"
        }
      }
    }
  }
}
```

这个示例的意思如下：

- 创建一个索引`pinyin`。

- 自定义一个分词器`my_pinyin`,也就是`tokenizer`的内容，该分词器类型是`pinyin`，关闭`pinyin`分词器`keep_first_letter`属性。

- 自定义一个分析器`pinyin_analyzer`，也就是`analyzer`下的内容。该分析器的分词器是我们前面自定义的分词器`my_pinyin`

测试示例代码如下：

```http
POST pinyin/_analyze
{
  "analyzer": "pinyin_analyzer",
  "text":"苹果"
}
```

其分词结果如下：

```json
{
  "tokens" : [
    {
      "token" : "ping",
      "start_offset" : 0,
      "end_offset" : 0,
      "type" : "word",
      "position" : 0
    },
    {
      "token" : "guo",
      "start_offset" : 0,
      "end_offset" : 0,
      "type" : "word",
      "position" : 1
    }
  ]
}
```

从结果可以看出，`pg`这个词已经不存在了，说明该效果就是由`keep_first_letter`控制的。

### keep_separate_first_letter

该属性的效果会将每个词的拼音首字母切分出来形成一个词，例如`苹果`->`p,g`。默认情况下，该效果没有开启。如果开启该效果，可能会导致查询结果太过于模糊。我们通过示例来查看效果。

自定义分词器如下：

```http
PUT pinyin
{
  "settings": {
    "analysis": {

      "tokenizer": {
        "my_pinyin":{
          "type":"pinyin",
          "keep_first_letter":true,
          "keep_separate_first_letter":true
        }
      },

      "analyzer": {
        "pinyin_analyzer":{
          "tokenizer":"my_pinyin"
        }
      }
    }
  }
}
```

注意之前已经创建了索引`pinyin`，在执行该语句前我们先通过下列语句删除索引，避免重复创建索引报错。

```http
DELETE pinyin
```

还是运行之前的示例代码，其分词效果如下:

```json
{
  "tokens" : [
    {
      "token" : "p",
      "start_offset" : 0,
      "end_offset" : 0,
      "type" : "word",
      "position" : 0
    },
    {
      "token" : "ping",
      "start_offset" : 0,
      "end_offset" : 0,
      "type" : "word",
      "position" : 0
    },
    {
      "token" : "pg",
      "start_offset" : 0,
      "end_offset" : 0,
      "type" : "word",
      "position" : 0
    },
    {
      "token" : "g",
      "start_offset" : 0,
      "end_offset" : 0,
      "type" : "word",
      "position" : 1
    },
    {
      "token" : "guo",
      "start_offset" : 0,
      "end_offset" : 0,
      "type" : "word",
      "position" : 1
    }
  ]
}
```

### limit_first_letter_length

该属性是用来控制`keep_first_letter`长度的，默认情况下支持的最大长度为16。

### keep_full_pinyin

是否保留全部拼音，默认情况下为`true`。如果设置为`false`，分词结果中将不会出现该词的全部拼音。

### keep_none_chinese

是否保留非中文字母或数字，默认值为`true`。我们先开一下开启的效果：

```http
POST pinyin/_analyze
{
  "analyzer": "pinyin_analyzer",
  "text":"ES搜索引擎"
}
```

分词效果如下：

```json
{
  "tokens" : [
    {
      "token" : "e",
      "start_offset" : 0,
      "end_offset" : 0,
      "type" : "word",
      "position" : 0
    },
    {
      "token" : "esssyq",
      "start_offset" : 0,
      "end_offset" : 0,
      "type" : "word",
      "position" : 0
    },
    {
      "token" : "s",
      "start_offset" : 0,
      "end_offset" : 0,
      "type" : "word",
      "position" : 1
    },
    {
      "token" : "sou",
      "start_offset" : 0,
      "end_offset" : 0,
      "type" : "word",
      "position" : 2
    },
    {
      "token" : "suo",
      "start_offset" : 0,
      "end_offset" : 0,
      "type" : "word",
      "position" : 3
    },
    {
      "token" : "yin",
      "start_offset" : 0,
      "end_offset" : 0,
      "type" : "word",
      "position" : 4
    },
    {
      "token" : "qing",
      "start_offset" : 0,
      "end_offset" : 0,
      "type" : "word",
      "position" : 5
    }
  ]
}
```

从分词结果可以看出`ES`被小写并被拆分成`e,s`两个词保留下来了。如果我们将`keep_none_chinese`设置为`false`，再次查看分词结果如下：

```json
{
  "tokens" : [
    {
      "token" : "sou",
      "start_offset" : 0,
      "end_offset" : 0,
      "type" : "word",
      "position" : 0
    },
    {
      "token" : "esssyq",
      "start_offset" : 0,
      "end_offset" : 0,
      "type" : "word",
      "position" : 0
    },
    {
      "token" : "suo",
      "start_offset" : 0,
      "end_offset" : 0,
      "type" : "word",
      "position" : 1
    },
    {
      "token" : "yin",
      "start_offset" : 0,
      "end_offset" : 0,
      "type" : "word",
      "position" : 2
    },
    {
      "token" : "qing",
      "start_offset" : 0,
      "end_offset" : 0,
      "type" : "word",
      "position" : 3
    }
  ]
}
```

从结果可以看出，`ES`的分词结果被去掉了未被保留。

### keep_none_chinese_together

该属性的效果是将非中文字母或数字放在一起。默认情况下该值为`true`。例如`ES搜索引擎`默认情况下结果为`e,s,sou,suo,yin,qing`，如果该值为`true`时，结果将变成`es,sou,suo,yin,qing`。

> 在试验的时候发现该值与`none_chinese_pinyin_tokenize`会冲突，需要将该值设置为false才会有效果。

### keep_none_chinese_in_first_letter

将非中文字母或者数字保留在首字母中，默认情况下为`true`。例如`es搜索引擎`默认情况下回保留在`esssyq`中，如果关闭则首字母中只会保留`ssyq`。

### keep_none_chinese_in_joined_full_pinyin

该属性的意思是将非中文内容文字和中文汉字拼音拼接到一起。默认情况下为`false`。例如`ES搜索引擎`，如果开启之后则会新增`essousuoyinqing`这个分词。

> 注意该属性需要开启`keep_joined_full_pinyin`属性才能观察到效果。

### none_chinese_pinyin_tokenize

该属性的意思是将非中文字母分解为单独的拼音术语。默认情况下该值为`true`。例如`搜索引擎`的拼音`sousuoyinqing`，开启该效果后则会分词成`sou,suo,yin,qing`。该特性需要`keep_none_chinese` 和 `keep_none_chinese_together`都开启才支持。

### keep_original

是否保留原文档语句，默认情况下为`false`。开启之后，例如`搜索引擎`该源内容会被单独形成一个分词结果`搜索引擎`。

### lowercase

分词之后的拼音是否小写。默认为`true`。

### trim_whitespace

是否去掉空白符。默认为`true`。

### remove_duplicated_term

是否移除重复的词。默认为`false`。例如`天天向上`最后的分词为`tian,xiang,shang`。

> 开启该属性可能导致位置相关的查询有问题。

### ignore_pinyin_offset

是否忽略拼音的偏移量。默认为`true`。在我们前面的示例中，分词结果中的`start_offset`和`end_offset`全部为0。开启的情况下会影响位置相关查询的，例如高亮可能不正确。关闭之后则会返回分词之后的偏移量，但是可能会导致写入数据失败。

## 使用

实际上我们很少直接使用`pinyin`分词器，更多的情况下我们可能会使用其字符过滤器。

## 小结

文本只是介绍了如何安装`pinyin`分词器已经其属性设置，后续将更新持续更新自定义分析器以及`ik`结合`pinyin`一起自定义分词器文章，实现既可以汉字搜索也可以拼音搜索的功能。
