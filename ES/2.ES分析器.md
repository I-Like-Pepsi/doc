# ES分析器

## 简介

在ES中不管是自定义的还是ES内置的分析器都由三个部分组成：字符过滤器、分词器、分词过滤器。

## 字符过滤器（Character filters）

字符过滤器的主要作用是接口原字符流，通过添加、删除或者替换操作改变原字符流。例如我们的文档内容可以是抓取的html内容，此时可以使用字符过滤器将文档中的html标签去掉。

在一个分析器中可以同时存在0个或者多个字符过滤器，按照它们的顺序应用。

```http
POST /_analyze
{
  "tokenizer": "keyword",
  "char_filter": ["html_strip"],
  "text": "<b>Hello world</b>"
}
```

上面我们通过`_analyze`API测试字符过滤器的效果，文本内容为带有html标签的内容，现在我们需要html去掉，我们通过ES提供的```html_strip```去掉html标签，结果如下：

```json
{
  "tokens" : [
    {
      "token" : "Hello world",
      "start_offset" : 3,
      "end_offset" : 18,
      "type" : "word",
      "position" : 0
    }
  ]
}
```

官方还提供了其他几种字符过滤器，具体参考官网文档：

> https://www.elastic.co/guide/en/elasticsearch/reference/7.1/analysis-charfilters.html

## 分词器(Tokenizer)

接收字符流然后将字符拆分成词，简单的说就是将一段文本拆分出来一个个词。例如对于英语来说，通过使用空格符将一个句子拆分成一个个词出来。

在一个分析器中，分词器有且只有一个。

```http
POST /_analyze
{
  "tokenizer": "standard",
  "char_filter": ["html_strip"],
  "text":"<b>Hello world</b>"
}
```

在前面讲字符过滤器的时候其实我们已经用过了分词器，它的分词器是`keyword`分词器，我们这里将其更换为`standard`分词器再次请求等到如下结果：

```json
{
  "tokens" : [
    {
      "token" : "Hello",
      "start_offset" : 3,
      "end_offset" : 8,
      "type" : "<ALPHANUM>",
      "position" : 0
    },
    {
      "token" : "world",
      "start_offset" : 9,
      "end_offset" : 18,
      "type" : "<ALPHANUM>",
      "position" : 1
    }
  ]
}
```

从结果可以看出，它通过空格符号将`Hello world`拆分成两个词了，这也是es默认的分词器。

ES内部提供了许多分词器，具体效果请参考官网文档。

> https://www.elastic.co/guide/en/elasticsearch/reference/7.1/analysis-tokenizers.html

## 分词过滤器(Token filters)

将拆分后的词进行添加、删除或者改变操作。例如将大写的英文单词转成小写的，或者删除停用词，或者添加近义词等。但是不允许修改词的position和offset等属性。

在一个分析器中，分词过滤器可以是0个或者多个。

```http
POST /_analyze
{
  "tokenizer": "standard",
  "char_filter": ["html_strip"],
  "text":"<b>Hello world</b>",
  "filter": ["lowercase"]
}
```

这个例子我们使用`lowercase`分词过滤器，该分词过滤器的作用是将大写转换成小写，例如`Hello`这个词，通过该分词器能将其转变成`hello`。具体结果如下：

```json
{
  "tokens" : [
    {
      "token" : "hello",
      "start_offset" : 3,
      "end_offset" : 8,
      "type" : "<ALPHANUM>",
      "position" : 0
    },
    {
      "token" : "world",
      "start_offset" : 9,
      "end_offset" : 18,
      "type" : "<ALPHANUM>",
      "position" : 1
    }
  ]
}
```

例如在英文中有很多停用词，像`and`、`this`等等，我们可以使用官方提供的`stop`字符过滤器将这些词去掉，只留下关键的词信息。

```json
POST /_analyze
{
  "tokenizer": "standard",
  "char_filter": ["html_strip"],
  "text":"<b>this is a Dog</b>",
  "filter": ["lowercase","stop"]
}
```

分词结果如下：

```json
{
  "tokens" : [
    {
      "token" : "dog",
      "start_offset" : 13,
      "end_offset" : 20,
      "type" : "<ALPHANUM>",
      "position" : 3
    }
  ]
}
```

从结果可以看出，停用词都被全部删除了，最后的结果只保留关键信息。官方给出了大量的分词过滤器，例如近义词相关的等，具体可以参考官方文档：

> https://www.elastic.co/guide/en/elasticsearch/reference/7.1/analysis-tokenfilters.html
