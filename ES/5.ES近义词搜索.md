# ES近义词搜索

## 简述

在实际生活中我们经常会碰见同一个物品有多个名字的情况，例如常见的`西红柿`，我们还可以叫`番茄`。用户在搜索时可能搜的是`西红柿`也可能搜的是`番茄`，针对这种情况在ES中该如何处理呢？

## 近义词分词过滤器

好在ES内部提供了近义词分词过滤器，通过它就能实现近义词转换。在ES中默认提供了两种近义词过滤器分别为：`Synonym Token Filter`和`Synonym Graph Token Filter`。具体这两种有什么区别后面再说，我们先讲讲如何使用。

```http
PUT /test_index
{
  "settings": {
    "analysis": {
      "filter": {
        "my_synonym":{
          "type":"synonym",
          "expand":true,
          "synonyms":["土豆,马铃薯","西红柿 => 番茄","china,中国,中华人民共和国"]
        }
      },
      "analyzer": {
        "my_synonym_analyzer":{
          "tokenizer":"ik_smart",
          "filter":["my_synonym"]
        }
      }
    }
  },
  "mappings": {
    "properties": {
      "content":{
        "type":"text",
        "analyzer": "my_synonym_analyzer"
      }
    }
  }
}
```

对上面的代码我这里简单的做一些解释：

- `analysis`这个是用来定义分析器相关内容的。

- `filter`这里面我们定义了一个`my_synonym`的分词过滤器，它的类型是`synonym`，并且我们这里配置了一些近义词。

- `analyzer`这里面就是我们定义了一个名叫`my_synonym_analyzer`的分析器，它的分词器是`ik_smart`，分词过滤器是`my_synonym`也就是在上面我们自定义的。

- `mappings`里面定义的是索引的字段`content`，它的类型是`text`且分析器用的是`my_synonym_analyzer`，也就是在前面我们自定义的。

> 关于分析器、分词器、分词过滤器、字符过滤器不太理解的可以关注公众号回复关键字“分析器”即可收到相关内容推送。

接下来我们向索引中写入几条数据。

```http
POST /test_index/_doc
{
  "content":"中华人民共和国历史悠久"
}

POST /test_index/_doc
{
  "content":"我爱吃西红柿"
}

POST /test_index/_doc
{
  "content":"土豆可以做薯片"
}
```

接下来我们分别搜索下面的关键字`中国、番茄、马铃薯`

```http
GET /test_index/_search
{
  "query": {
    "match": {
      "content": "中国、番茄、马铃薯"
    }
  }
}
```

其响应结果如下：

```json
{
  "took" : 1,
  "timed_out" : false,
  "_shards" : {
    "total" : 1,
    "successful" : 1,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : {
      "value" : 3,
      "relation" : "eq"
    },
    "max_score" : 1.7423426,
    "hits" : [
      {
        "_index" : "test_index",
        "_type" : "_doc",
        "_id" : "NDVVWoQBVaeBvu5UD9dR",
        "_score" : 1.7423426,
        "_source" : {
          "content" : "中华人民共和国历史悠久"
        }
      },
      {
        "_index" : "test_index",
        "_type" : "_doc",
        "_id" : "jzVVWoQBVaeBvu5Uo9d0",
        "_score" : 1.2927058,
        "_source" : {
          "content" : "土豆可以做薯片"
        }
      },
      {
        "_index" : "test_index",
        "_type" : "_doc",
        "_id" : "NTVVWoQBVaeBvu5UHtcq",
        "_score" : 1.1220685,
        "_source" : {
          "content" : "我爱吃西红柿"
        }
      }
    ]
  }
}
```

从结果可以看出，我们通过近义词能将刚刚我们写入的数据全部搜索出来。

### expand属性的作用

在刚刚我们创建`my_synonym`分词过滤器的时候我们将`expand`属性设置成true，默认情况下该值也是true，那么这个值有什么作用呢？我们先来通过`_analyze`测试一下我们分词的效果。

```http
POST /test_index/_analyze
{
  "analyzer": "my_synonym_analyzer",
  "text":"我爱吃土豆"
}
```

分析结果如下：

```json
{
  "tokens" : [
    {
      "token" : "我",
      "start_offset" : 0,
      "end_offset" : 1,
      "type" : "CN_CHAR",
      "position" : 0
    },
    {
      "token" : "爱吃",
      "start_offset" : 1,
      "end_offset" : 3,
      "type" : "CN_WORD",
      "position" : 1
    },
    {
      "token" : "土豆",
      "start_offset" : 3,
      "end_offset" : 5,
      "type" : "CN_WORD",
      "position" : 2
    },
    {
      "token" : "马铃薯",
      "start_offset" : 3,
      "end_offset" : 5,
      "type" : "SYNONYM",
      "position" : 2
    }
  ]
}
```

从结果可以看出，它会将`土豆`的近义词添加到分词结果中。而`expand`的作用就是将近义词添加到分词结果中做索引，如果该值为`false`，那么分词结果中将不会存在词的近义词。

### synonyms

该属性是用来配置近义词的。但是我们的示例中有两种配置方式，他们有何不同的？

- `近义词1,近义词2,近义词3`

- `近义词1 => 近义词2`

对于第一种方式，`近义词1,近义词2,近义词3`这三个词是可以互相相等的。如果对`近义词1`进行分词，其结果中会包含`近义词2,近义词3`分词结果。

但是对于第二种方式，`近义词1 => 近义词2`这个代表`近义词1`在分词后会被替换成`近义词2`。我们直接看个示例就知道了。

```http
POST /test_index/_analyze
{
  "analyzer": "my_synonym_analyzer",
  "text":"我爱吃西红柿"
}

POST /test_index/_analyze
{
  "analyzer": "my_synonym_analyzer",
  "text":"我爱吃番茄"
}
```

对这个两个文本做分词，其结果分别如下：

```json
{
  "tokens" : [
    {
      "token" : "我",
      "start_offset" : 0,
      "end_offset" : 1,
      "type" : "CN_CHAR",
      "position" : 0
    },
    {
      "token" : "爱吃",
      "start_offset" : 1,
      "end_offset" : 3,
      "type" : "CN_WORD",
      "position" : 1
    },
    {
      "token" : "番茄",
      "start_offset" : 3,
      "end_offset" : 6,
      "type" : "SYNONYM",
      "position" : 2
    }
  ]
}
```

```json
{
  "tokens" : [
    {
      "token" : "我",
      "start_offset" : 0,
      "end_offset" : 1,
      "type" : "CN_CHAR",
      "position" : 0
    },
    {
      "token" : "爱吃",
      "start_offset" : 1,
      "end_offset" : 3,
      "type" : "CN_WORD",
      "position" : 1
    },
    {
      "token" : "番茄",
      "start_offset" : 3,
      "end_offset" : 5,
      "type" : "CN_WORD",
      "position" : 2
    }
  ]
}
```

从结果一可以看出，`西红柿`被替换成了`番茄`，但是结果二中，`番茄`并没有被改变。

### synonyms_path

前面介绍了配置`synonyms`用来配置近义词，但是实际生产中我们可能需要维护成千上万个近义词。那么`synonyms`这种方式很显然没办法满足我们的要求。而`synonyms_path`的作用就是用来配置近义词文件位置的。我们可以将近义词规则写入到文件中，然后通过`synonyms_path`指定该文件位置即可。

> synonyms_path中的位置是相对`${ES_HOME}/config`目录的。

接下来我们在`${ES_HOME}/config`下创建`analysis/synonym.txt`文件，该文件具体是什么名字没有限制要求，然后在文件中配置如下近义词内容：

```
腾讯,tencent
网易,netease
```

接着我们删除之前创建的索引，重建索引如下：

```http
PUT /test_index
{
  "settings": {
    "analysis": {
      "filter": {
        "my_synonym":{
          "type":"synonym",
          "expand":true,
          "synonyms":["土豆,马铃薯","西红柿 => 番茄","china,中国,中华人民共和国"],
          "synonyms_path":"analysis/synonym.txt"
        }
      },
      "analyzer": {
        "my_synonym_analyzer":{
          "tokenizer":"ik_smart",
          "filter":["my_synonym"]
        }
      }
    }
  },
  "mappings": {
    "properties": {
      "content":{
        "type":"text",
        "analyzer": "my_synonym_analyzer"
      }
    }
  }
}
```

这里我们将`synonyms_path`的值设置成我们先前配置的近义词文件的位置，然后我们可以尝试建立如下几条索引：

```http
POST /test_index/_doc
{
  "content":"网易是国内最早一批互联网公司"
}

POST /test_index/_doc
{
  "content":"腾讯是国内游戏大厂"
}
```

接下啦测试搜索英文如下：

```json
GET /test_index/_search
{
  "query": {
    "match": {
      "content": "tencent and netease"
    }
  }
}
```

结果是能搜索到前面创建的文档数据。

#### 新增词如何生效

如果我们在`analysis/synonym.txt`新增加了近义词，但是发现不生效该怎么办？对于新增加的近义词是不会生效的，我们需要对索引进行关闭之后然后再次打开即可生效，命令如下：

```http
POST /${索引名}/_close

POST /${索引名}/_open
```

## Synonym Token Filter和Synonym Graph Token Filter的区别

在官网上可以看见有两个近义词分词过滤器，它们的用法基本上都是一样的。不过文档中只有一段文字说了它们之间的不同，`Synonym Token Filter`通常用作索引时，而`Synonym Graph Token Filter`用在搜索时。

首先需要明白一个道理，并不是只有在索引文档时才会对字段进行分析，搜索时同样可能会对搜索的文本进行分析。在大多数情况下，索引和搜索时的分析器是一致的，但实际上它们可以被设置成不同。例如对于索引文档时我并不想让近义词添加到我的分词结果，但是实际搜索时我又想实现近义词搜索。

例如`我爱吃西红柿`，索引文档时结果只有`我,爱吃,西红柿`。但是在搜索时我输入的是`番茄`，但是在搜索时实际上会去搜索`番茄,西红柿`这两个词，这样即使我们的数据中没有`番茄`这个词，我们的文档还是能被搜索到。下面我们通过一个示例来演示：

```http
PUT /test_index
{
  "settings": {
    "analysis": {
      "filter": {
        "my_synonym":{
          "type":"synonym_graph",
          "expand":true,
          "synonyms":["西红柿,番茄"]
        }
      },
      "analyzer": {
        "my_synonym_analyzer":{
          "tokenizer":"ik_smart",
          "filter":["my_synonym"]
        }
      }
    }
  },
  "mappings": {
    "properties": {
      "content":{
        "type":"text",
        "analyzer": "ik_smart",
        "search_analyzer": "my_synonym_analyzer"
      }
    }
  }
}
```

注意这里我们设置了`content`字段索引时的分析器是`ik_smart`，但是搜索时是我们自定义的`my_synonym_analyzer`。实际上相比于`ik_smart`，我们自定义的分析器多了近义词的功能。

```http
POST /test_index/_doc
{
  "content":"我爱吃西红柿"
}

GET /test_index/_search
{
  "query": {
    "match": {
      "content": "番茄"
    }
  }
}

{
  "took" : 0,
  "timed_out" : false,
  "_shards" : {
    "total" : 1,
    "successful" : 1,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : {
      "value" : 1,
      "relation" : "eq"
    },
    "max_score" : 0.2876821,
    "hits" : [
      {
        "_index" : "test_index",
        "_type" : "_doc",
        "_id" : "dzapWoQBVaeBvu5UmQcC",
        "_score" : 0.2876821,
        "_source" : {
          "content" : "我爱吃西红柿"
        }
      }
    ]
  }
}
```

从最后结果可以看出，即使索引的数据中不存在`番茄`我们还是能搜索出其近义词相关的内容。
